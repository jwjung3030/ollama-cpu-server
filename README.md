# Ollama CPU Server

ì´ ì €ì¥ì†ŒëŠ” **[Ollama](https://github.com/ollama/ollama)** ëŸ°íƒ€ì„ ìœ„ì—ì„œ **Google Gemma 3** ëª¨ë¸ì„ CPU í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ì—¬  
ê°„ë‹¨í•œ REST APIë¥¼ êµ¬ì¶•í•˜ëŠ” ì˜ˆì œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸš€ íŠ¹ì§•
- **CPU ê¸°ë°˜ ì‹¤í–‰**: GPU ì—†ì´ë„ Gemma 3 ëª¨ë¸ êµ¬ë™ ê°€ëŠ¥ (ëŠë¦´ ìˆ˜ ìˆìŒ).
- **ê°„ë‹¨í•œ ë°°í¬**: `docker-compose up` ë§Œìœ¼ë¡œ ì„œë²„ ì‹¤í–‰.
- **REST API ì§€ì›**: `curl` ë˜ëŠ” ë‹¤ë¥¸ í´ë¼ì´ì–¸íŠ¸ë¡œ ë²ˆì—­ ìš”ì²­ ê°€ëŠ¥.
---

## ğŸ“¦ ì„¤ì¹˜ ë°©ë²•

### 1. ì €ì¥ì†Œ í´ë¡ 
```bash
git clone https://github.com/jwjung3030/ollama-cpu-server
cd ollama-cpu-server

```

### 2. ê¸°ë™ & ëª¨ë¸ í’€
```bash
docker compose up -d
docker exec -it gemma3-ollama ollama pull gemma3:4b

```

### 3. REST í˜¸ì¶œ (Ollama Chat API)
```bash
curl http://localhost:11434/api/chat -d '{
  "model":"gemma3:4b",
  "messages":[
    {"role":"system","content":"You are a translation engine. Output only the translation."},
    {"role":"user","content":"í•œêµ­ì–´ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´ë¡œ ë²ˆì—­: ìš°ë¦¬ëŠ” ìƒ‰ë³´ì •ì„ ìë™í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤."}
  ],
  "options":{"temperature":0.2}
}'
